{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AN1_bio_saint_base_build_MHA_8_CLB_FinalRun_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"eJLYCRSoGrgy","executionInfo":{"status":"ok","timestamp":1637741290372,"user_tz":-360,"elapsed":555,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["save_path = '/content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7bmIj0V7b2W","executionInfo":{"status":"ok","timestamp":1637741096673,"user_tz":-360,"elapsed":20095,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"6afd3aa3-5c90-4a4e-e41d-3ba79d3350bf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TB_93aPdSeDC","executionInfo":{"status":"ok","timestamp":1637741292211,"user_tz":-360,"elapsed":4,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"62c4274b-39b8-48bd-a5d4-327bd1d59578"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov 24 08:08:11 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ugf4FLiJI0iR","executionInfo":{"status":"ok","timestamp":1637741353495,"user_tz":-360,"elapsed":58221,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"647ad6ff-9f12-4001-b69d-64adeaa6d007"},"source":["import shutil\n","shutil.copyfile('/content/drive/MyDrive/bio-data/ok.zip', './ok.zip')\n","!unzip ok.zip\n","\n","shutil.copyfile('/content/drive/MyDrive/bio-data/training-needs.zip', './train.zip')\n","!unzip train.zip"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  ok.zip\n","   creating: Test/\n","  inflating: Test/5G6UA.fasta        \n","  inflating: Test/5G6UA.hhm          \n","  inflating: Test/5G6UA.pssm         \n","  inflating: Test/5G6UA.spotcon      \n","  inflating: Test/5G6UA.ss           \n","  inflating: Test/5H7KA.fasta        \n","  inflating: Test/5H7KA.hhm          \n","  inflating: Test/5H7KA.pssm         \n","  inflating: Test/5H7KA.spotcon      \n","  inflating: Test/5H7KA.ss           \n","  inflating: 0norm_data.p            \n","  inflating: 1norm_data.p            \n","  inflating: 5norm_data.p            \n","  inflating: aa_phy7                 \n","  inflating: config.py               \n","  inflating: GenerateBaseFeature.py  \n","  inflating: GenerateForWindow10.py  \n","  inflating: GenerateForWindow20.py  \n","  inflating: GenerateForWindow50.py  \n","  inflating: GenerateLengthList.py   \n","  inflating: list_test               \n","  inflating: SAINT_ensemble.py       \n","  inflating: SAINT_single_base_model.py  \n","Archive:  train.zip\n","   creating: training-needs/\n","  inflating: training-needs/trainX.npy  \n","  inflating: training-needs/validationX.npy  \n","  inflating: training-needs/test2016X.npy  \n","  inflating: training-needs/test2018X.npy  \n","  inflating: training-needs/trainY.npy  \n","  inflating: training-needs/validationY.npy  \n","  inflating: training-needs/test2016Y.npy  \n","  inflating: training-needs/test2018Y.npy  \n","  inflating: training-needs/trainAttM.npy  \n","  inflating: training-needs/validationAttM.npy  \n","  inflating: training-needs/test2016AttM.npy  \n","  inflating: training-needs/test2018AttM.npy  \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-i3x_TOo70IO","executionInfo":{"status":"ok","timestamp":1637741354166,"user_tz":-360,"elapsed":676,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"633590f5-3596-4c59-ff23-a3d8c02b99b1"},"source":["########### Select Tensorflow and Keras Version\n","%tensorflow_version 1.x"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJ2i1WAu9A1P","executionInfo":{"status":"ok","timestamp":1637741356612,"user_tz":-360,"elapsed":2447,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"e2d8245a-e04d-46ae-9f1d-62d4f3fcd1e2"},"source":["########## imports\n","# tensorflow and keras functions\n","import tensorflow as tensorflow\n","import tensorflow as tf\n","import keras\n","from keras import backend as K\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras.layers.embeddings import Embedding\n","from keras.layers.recurrent import LSTM, GRU\n","from keras.layers import Input,SpatialDropout1D, Embedding, LSTM, Dense, merge, Convolution2D, Lambda, GRU, TimeDistributed, Reshape, Permute, Convolution1D, Masking, Bidirectional\n","from keras.optimizers import Adam\n","from keras.layers import concatenate\n","from keras.regularizers import l2\n","from keras.callbacks import ModelCheckpoint\n","from keras import optimizers, callbacks\n","from keras.layers import Layer\n","from keras.initializers import Ones, Zeros\n","from keras.layers import Layer\n","from keras.engine.topology import Layer\n","from keras.layers import add\n","from keras.layers import RepeatVector, Multiply, Flatten, Dot, Softmax, Lambda\n","from keras import backend\n","from keras.layers import RepeatVector\n","from keras.layers import Embedding, Add, concatenate\n","from keras.layers import BatchNormalization, Dropout\n","\n","# python built-in\n","from time import time\n","import numpy as np\n","import gc\n","import random\n","import math\n","\n","# custom \n","import config\n","from config import *\n","from GenerateBaseFeature import generateBaseDataset\n","from GenerateLengthList import generateLengthlistAndAttentionMask\n","\n","print(tensorflow.__version__)\n","print(keras.__version__)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["1.15.2\n","2.3.1\n"]},{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","metadata":{"id":"2Oz3QPA49VH5","executionInfo":{"status":"ok","timestamp":1637742145845,"user_tz":-360,"elapsed":1638,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["def truncated_accuracy(y_true, y_predict):\n","    mask = K.sum(y_true, axis=2)\n","    y_pred_labels = K.cast(K.argmax(y_predict, axis=2), 'float32')\n","    y_true_labels = K.cast(K.argmax(y_true, axis=2), 'float32')\n","    is_same = K.cast(K.equal(\n","        y_true_labels, y_pred_labels), 'float32')\n","    num_same = K.sum(is_same * mask, axis=1)\n","    lengths = K.sum(mask, axis=1)\n","    # return K.cast(K.sum(num_same, axis=0) / K.sum(lengths, axis=0), 'float32')\n","    return Lambda(lambda x: x[0]/x[1])([K.sum(num_same, axis=0), K.sum(lengths, axis=0)])\n","\n","class LayerNormalization(Layer):\n","    def __init__(self, eps: float = 1e-5, **kwargs) -> None:\n","        self.eps = eps\n","        super().__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n","        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n","        super().build(input_shape)\n","\n","    def call(self, x, **kwargs):\n","        u = K.mean(x, axis=-1, keepdims=True)\n","        s = K.mean(K.square(x - u), axis=-1, keepdims=True)\n","        z = (x - u) / K.sqrt(s + self.eps)\n","        return self.gamma * z + self.beta\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    def get_config(self):\n","        config = {\n","            'eps': self.eps,\n","        }\n","        base_config = super().get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class MyLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(MyLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","\n","        self._x = K.variable(0.2)\n","        self._x._trainable = True\n","        self.trainable_weights = [self._x]\n","\n","        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n","\n","    def call(self, x):\n","        A, B = x\n","        result = add([self._x*A ,(1-self._x)*B])\n","        return result\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0]\n","\n","def attention(activations):\n","#https://arxiv.org/pdf/1703.03130.pdf\n","    d_a = 10\n","    r = 1\n","    units = activations.shape[2]\n","    #print(activations.shape)\n","    attention = TimeDistributed(Dense(d_a, activation='tanh', use_bias=False))(activations)\n","    attention = Dropout(.5)(attention)\n","    #print(attention.shape)\n","    attention = TimeDistributed(Dense(r, activation='softmax', use_bias=False))(activations) \n","    #print(attention.shape)\n","    attention = Flatten()(attention)\n","    #attention = Activation('softmax')(attention)\n","    attention = RepeatVector(units)(attention)\n","    #print(attention.shape)\n","    attention = Permute([2, 1])(attention)\n","    #print(attention.shape)\n","\n","    # apply the attention\n","    sent_representation = Multiply()([activations, attention])\n","    #sent_representation = Lambda(lambda xin: K.dot(xin[0], xin[1]))([attention, activations])\n","    #print(sent_representation.shape)\n","    #sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n","    #print(sent_representation.shape)\n","    return sent_representation\n","\n","def shape_list(x):\n","    #   https://github.com/Separius/BERT-keras/blob/master/transformer/funcs.py\n","    if backend.backend() != 'theano':\n","        tmp = backend.int_shape(x)\n","    else:\n","        tmp = x.shape\n","    tmp = list(tmp)\n","    tmp[0] = -1\n","    return tmp\n","\n","def attention_scaled_dot(activations, attention_mask, head = 1): #, length):\n","#https://arxiv.org/pdf/1706.03762.pdf\n","    units = int(activations.shape[2])\n","    words = int(activations.shape[1])\n","    print('Activations.shape: ',activations.shape)\n","    _drop_rate_ = .1\n","    hidden_dimension = int(units/head)\n","    Q = TimeDistributed(Dense(hidden_dimension, activation=None, use_bias=False))(activations)\n","    Q = Dropout(_drop_rate_)(Q)\n","    K = TimeDistributed(Dense(hidden_dimension, activation=None, use_bias=False))(activations)\n","    K = Dropout(_drop_rate_)(K)\n","    V = TimeDistributed(Dense(hidden_dimension, activation=None, use_bias=False))(activations)\n","    V = Dropout(_drop_rate_)(V)\n","    print('Q.shape: ',Q.shape)\n","    QK_T = Dot(axes=-1, normalize=False)([Q,K]) # list of two tensors\n","    print('QK_T.shape: ', QK_T.shape)\n","    \"\"\"normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.\"\"\"\n","    QK_T = Lambda( lambda inp: inp[0]/ backend.sqrt(backend.cast(shape_list(inp[1])[-1], backend.floatx())))([QK_T, V])\n","    print('QK_T.shape: ', QK_T.shape)\n","    \n","#     cropping = np.zeros(QK_T.shape[1])\n","#     cropping[length:] = (-10**6) * np.ones(int(QK_T.shape[1])-length)\n","#     QK_T = QK_T + cropping\n","    print('att mask origin shape: ',attention_mask.shape)\n","    attention_mask__ = RepeatVector(int(QK_T.shape[1]))(attention_mask)\n","    print('att mask shape: ',attention_mask__.shape)\n","    QK_T = Add()([QK_T, attention_mask__])\n","    print('Add QK and mask: ',attention_mask__.shape)\n","    QK_T = Softmax(axis=-1)(QK_T)\n","    QK_T = Dropout(_drop_rate_)(QK_T)\n","    #print(V.shape)\n","    V = Permute([2, 1])(V)\n","    print('V premute shape: ', V.shape)\n","    V_prime = Dot(axes=-1, normalize=False)([QK_T,V]) # list of two tensors\n","    print('V_prime shape: ', V_prime.shape)\n","    return V_prime\n","\n","def _get_pos_encoding_matrix(protein_len: int, d_emb: int) -> np.array:\n","    pos_enc = np.array(\n","        [[pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] if pos != 0 else np.zeros(d_emb) for pos in\n","         range(protein_len)], dtype=np.float32)\n","    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n","    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n","    return pos_enc\n","\n","def embeddings(inputs):\n","    gene_ids, pos_ids = inputs\n","    gene_vocab_len = 22\n","    protein_len = 700\n","    output_dim = 50\n","    gene_embedding = Embedding(gene_vocab_len, output_dim, input_length=protein_len,\n","                                              name='GeneEmbedding')(gene_ids)\n","\n","    pos_embedding = Embedding(protein_len, output_dim, trainable=False, input_length=protein_len,\n","                                              name='PositionEmbedding',\n","                                              weights=[_get_pos_encoding_matrix(protein_len, output_dim)])(pos_ids)\n","\n","    summation = Add(name='AddEmbeddings')([Dropout(.1, name='EmbeddingDropOut1')(gene_embedding), \n","                                           Dropout(.1, name='EmbeddingDropOut2')(pos_embedding)])\n","    \n","#     summation = concatenate([Dropout(.1, name='EmbeddingDropOut1')(gene_embedding), \n","#                              Dropout(.1, name='EmbeddingDropOut2')(pos_embedding)])\n","    \n","    summation = LayerNormalization(1e-5)(summation)\n","    return summation\n","\n","def gene_embeddings(gene_ids, output_dim=50):\n","    gene_vocab_len = 22\n","    protein_len = 700\n","    \n","    gene_emb = Dropout(.1)(Embedding(gene_vocab_len, output_dim, input_length=protein_len,\n","                               name='GeneEmbedding')(gene_ids))\n","\n","    gene_emb = LayerNormalization(1e-5)(gene_emb)\n","    return gene_emb\n","\n","\n","def position_embedding(pos_ids, output_dim=50):\n","    #gene_vocab_len = 22\n","    protein_len = 700\n","    output_dim = int(output_dim)\n","\n","    pos_emb = Dropout(.1)(Embedding(protein_len, output_dim, trainable=False, input_length=protein_len,\n","                        #name='PositionEmbedding',\n","                        weights=[_get_pos_encoding_matrix(protein_len, output_dim)])(pos_ids))\n","    \n","    pos_emb = LayerNormalization(1e-5)(pos_emb)\n","    return pos_emb\n","\n","\n","def attention_module(x, attention_mask, pos_ids=None, drop_rate=.1, ccnt=None, head = 1):\n","    \n","    original_dim = int(x.shape[-1])\n","    \n","    if ccnt is not None:\n","        print('aatt--------x1-> '+str(x.shape))\n","        print('original dim--------x1-> '+str(original_dim))\n","    if pos_ids is not None:\n","        pos_embedding = position_embedding(pos_ids=pos_ids, output_dim=original_dim)\n","        #x = concatenate([x, pos_embedding])\n","        x = Add()([x, pos_embedding])\n","\n","    if ccnt is not None:\n","        print('aatt--------x2-> '+str(x.shape))\n","\n","    \n","    att_layer_head1 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head2 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head3 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head4 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head5 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head6 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head7 = attention_scaled_dot(x, attention_mask, head) ## our code\n","    att_layer_head8 = attention_scaled_dot(x, attention_mask, head) ## our code\n","\n","    att_layer = keras.layers.Concatenate(axis = -1)([att_layer_head1,att_layer_head2,att_layer_head3,att_layer_head4,att_layer_head5,att_layer_head6,att_layer_head7,att_layer_head8]) ## our code\n","    if ccnt is not None:\n","        print('aatt--------att_layer1-> '+str(att_layer.shape))\n","    att_layer = Dropout(drop_rate)(att_layer)\n","\n","    att_layer = TimeDistributed(Dense(original_dim, activation=None, use_bias=False))(att_layer) ## our code\n","\n","    if ccnt is not None:\n","        print('aatt--------att_layer2-> '+str(att_layer.shape))\n","    x = MyLayer()([att_layer, x])\n","\n","    if ccnt is not None:\n","        print('aatt--------my-> '+str(x.shape))\n","    x = Dropout(drop_rate)(x)\n","    x = BatchNormalization()(x)\n","#     if False:\n","#         # reduce dim\n","#         x = Convolution1D(original_dim, 1, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n","#         x = Dropout(drop_rate*2)(x)\n","#         x = BatchNormalization()(x)\n","    # ccnt +=1\n","    return x\n","\n","def inceptionBlock(x):\n","    _drop_rate_ = .1\n","    x = BatchNormalization()(x)\n","    print('inceptionblock before CONV '+ str(x.shape))\n","    conv1_1 = Convolution1D(100, 1, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n","    print('inceptionblock after CONV '+ str(conv1_1.shape))\n","    conv1_1 = Dropout(_drop_rate_)(conv1_1) #https://www.quora.com/Can-l-combine-dropout-and-l2-regularization\n","    conv1_1 = BatchNormalization()(conv1_1)\n","    \n","    conv2_1 = Convolution1D(100, 1, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n","    conv2_1 = Dropout(_drop_rate_)(conv2_1)\n","    conv2_1 = BatchNormalization()(conv2_1)\n","    conv2_2 = Convolution1D(100, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv2_1)\n","    conv2_2 = Dropout(_drop_rate_)(conv2_2)\n","    conv2_2 = BatchNormalization()(conv2_2)\n","    \n","    conv3_1 = Convolution1D(100, 1, activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n","    conv3_1 = Dropout(_drop_rate_)(conv3_1)\n","    conv3_1 = BatchNormalization()(conv3_1)\n","    conv3_2 = Convolution1D(100, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv3_1)\n","    conv3_2 = Dropout(_drop_rate_)(conv3_2)\n","    conv3_2 = BatchNormalization()(conv3_2)\n","    conv3_3 = Convolution1D(100, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv3_2)\n","    conv3_3 = Dropout(_drop_rate_)(conv3_3)\n","    conv3_3 = BatchNormalization()(conv3_3)\n","    conv3_4 = Convolution1D(100, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv3_3)\n","    conv3_4 = Dropout(_drop_rate_)(conv3_4)\n","    conv3_4 = BatchNormalization()(conv3_4)\n","    \n","    concat = concatenate([conv1_1, conv2_2, conv3_4])\n","    concat = BatchNormalization()(concat)\n","    \n","    return concat\n","\n","def deep3iBLock_with_attention(x, attention_mask, pos_ids=None):\n","    print('x-> '+str(x.shape))\n","    block1_1 = inceptionBlock(x)\n","    print('block1_1-> '+str(block1_1.shape))\n","    block1_1 = attention_module(block1_1, attention_mask, pos_ids,ccnt=1, head = 8)\n","    print('block1_1-> '+str(block1_1.shape))\n","\n","    block2_1 = inceptionBlock(x)\n","    print('block2_1-> '+str(block2_1.shape))\n","    block2_2 = inceptionBlock(block2_1)\n","    print('block2_1-> '+str(block2_1.shape))\n","    block2_2 = attention_module(block2_2, attention_mask, pos_ids, head = 8)\n","    print('block2_2-> '+str(block2_2.shape))\n","\n","\n","    block3_1 = inceptionBlock(x)\n","    block3_2 = inceptionBlock(block3_1)\n","    block3_3 = inceptionBlock(block3_2)\n","    block3_4 = inceptionBlock(block3_3)\n","    print('block3_4-> '+str(block3_4.shape))\n","    block3_4 = attention_module(block3_4, attention_mask, pos_ids, head = 8)\n","    print('block3_4-> '+str(block3_4.shape))\n","\n","    concat = concatenate([block1_1, block2_2, block3_4])\n","    concat = BatchNormalization()(concat)\n","    \n","    return concat\n","\n","def get_model(num_feature):\n","#   pssm_input = Input(shape=(700,21,), name='pssm_input')\n","#   seq_input = Input(shape=(700,22,), name='seq_input')\n","  _drop_rate_ = .4\n","  main_input = Input(shape=(700,num_feature,), name='main_input')\n","  attention_mask = Input(shape=(700,), name='attention_mask')\n","  pos_ids = Input(shape=(700,), name='position_input', dtype='int32')\n","  \n","  #pos_emb = position_embedding(pos_ids, output_dim=50)\n","#   main_input = concatenate([seq_input, pssm_input])\n","  \n","  block1 = deep3iBLock_with_attention(main_input, attention_mask, pos_ids)\n","#   att_layer_4 = attention_scaled_dot(block1)\n","#   block1 = MyLayer()([att_layer_4 ,block1])\n","#   block1 = BatchNormalization()(block1)\n","  \n","  block2 = deep3iBLock_with_attention(block1, attention_mask, pos_ids)\n","  block2 = attention_module(block2, attention_mask, pos_ids, head = 8)\n","  \n","  conv11 = Convolution1D(100, 11, activation='relu', padding='same', kernel_regularizer=l2(0.001))(block2)\n","#   conv11 = BatchNormalization()(conv11)\n","  conv11 = attention_module(conv11, attention_mask, pos_ids, head = 8)\n","\n","  dense1 = TimeDistributed(Dense(units=256,activation='relu'))(conv11)\n","  dense1 = Dropout(_drop_rate_)(dense1)\n","  dense1 = attention_module(dense1, attention_mask, pos_ids, head = 8)\n","  \n","  print('before final: -------------------' + str(dense1.shape))\n","  main_output = TimeDistributed(Dense(units=8,activation='softmax', name='main_output'))(dense1)\n","  print('after final: -------------------' + str(main_output.shape))\n","#   main_output = TimeDistributed(Dense(units=8,activation='softmax', name='main_output'))(block1)\n","  \n","  model = Model([main_input, attention_mask, pos_ids],main_output)\n","  return model\n","\n","class StepDecay():\n","  def __init__(self, initAlpha=0.0005, factor=0.9, dropEvery=60, min_lr=0.00001):\n","    self.initAlpha = initAlpha\n","    self.factor = factor\n","    self.dropEvery = dropEvery\n","    self.min_lr = min_lr\n","\n","  def __call__(self, epoch):\n","    exp = np.floor((epoch + 1) / self.dropEvery)\n","    alpha = self.initAlpha * (self.factor ** exp)\n","    if float(alpha) > self.min_lr:\n","        print('lr:', alpha)\n","        return float(alpha)\n","    else:\n","        print('lr:', self.min_lr)\n","        return self.min_lr\n","    \n","def get_lr_metric(optimizer):\n","    def lr(y_true, y_pred):\n","        return optimizer.lr\n","    return lr\n","    "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdBpv0vtD0nk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637742194499,"user_tz":-360,"elapsed":46831,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"45d92d69-5c92-480d-9954-44ad74a2bec8"},"source":["model = get_model(num_feature=57)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["x-> (?, 700, 57)\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","inceptionblock before CONV (?, 700, 57)\n","inceptionblock after CONV (?, 700, 100)\n","block1_1-> (?, 700, 300)\n","aatt--------x1-> (?, 700, 300)\n","original dim--------x1-> 300\n","aatt--------x2-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","aatt--------att_layer1-> (?, 700, 296)\n","aatt--------att_layer2-> (?, 700, 300)\n","tracking <tf.Variable 'my_layer_1/Variable:0' shape=() dtype=float32> _x\n","aatt--------my-> (?, 700, 300)\n","block1_1-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 57)\n","inceptionblock after CONV (?, 700, 100)\n","block2_1-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","block2_1-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","tracking <tf.Variable 'my_layer_2/Variable:0' shape=() dtype=float32> _x\n","block2_2-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 57)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","block3_4-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","tracking <tf.Variable 'my_layer_3/Variable:0' shape=() dtype=float32> _x\n","block3_4-> (?, 700, 300)\n","x-> (?, 700, 900)\n","inceptionblock before CONV (?, 700, 900)\n","inceptionblock after CONV (?, 700, 100)\n","block1_1-> (?, 700, 300)\n","aatt--------x1-> (?, 700, 300)\n","original dim--------x1-> 300\n","aatt--------x2-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","aatt--------att_layer1-> (?, 700, 296)\n","aatt--------att_layer2-> (?, 700, 300)\n","tracking <tf.Variable 'my_layer_4/Variable:0' shape=() dtype=float32> _x\n","aatt--------my-> (?, 700, 300)\n","block1_1-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 900)\n","inceptionblock after CONV (?, 700, 100)\n","block2_1-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","block2_1-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","tracking <tf.Variable 'my_layer_5/Variable:0' shape=() dtype=float32> _x\n","block2_2-> (?, 700, 300)\n","inceptionblock before CONV (?, 700, 900)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","inceptionblock before CONV (?, 700, 300)\n","inceptionblock after CONV (?, 700, 100)\n","block3_4-> (?, 700, 300)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","Activations.shape:  (?, 700, 300)\n","Q.shape:  (?, 700, 37)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 37, 700)\n","V_prime shape:  (?, 700, 37)\n","tracking <tf.Variable 'my_layer_6/Variable:0' shape=() dtype=float32> _x\n","block3_4-> (?, 700, 300)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","Activations.shape:  (?, 700, 900)\n","Q.shape:  (?, 700, 112)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 112, 700)\n","V_prime shape:  (?, 700, 112)\n","tracking <tf.Variable 'my_layer_7/Variable:0' shape=() dtype=float32> _x\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","Activations.shape:  (?, 700, 100)\n","Q.shape:  (?, 700, 12)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 12, 700)\n","V_prime shape:  (?, 700, 12)\n","tracking <tf.Variable 'my_layer_8/Variable:0' shape=() dtype=float32> _x\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","Activations.shape:  (?, 700, 256)\n","Q.shape:  (?, 700, 32)\n","QK_T.shape:  (?, 700, 700)\n","QK_T.shape:  (?, 700, 700)\n","att mask origin shape:  (?, 700)\n","att mask shape:  (?, 700, 700)\n","Add QK and mask:  (?, 700, 700)\n","V premute shape:  (?, 32, 700)\n","V_prime shape:  (?, 700, 32)\n","tracking <tf.Variable 'my_layer_9/Variable:0' shape=() dtype=float32> _x\n","before final: -------------------(?, 700, 256)\n","after final: -------------------(?, 700, 8)\n"]}]},{"cell_type":"code","metadata":{"id":"yCBvyfAfD3yi"},"source":["# !pip install 'h5py==2.10.0' --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_CuGxQ-x8vj"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFyxYbSsK3ow"},"source":["# model.load_weights('/content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-2-CLB-FinalRun-1/best-0.757412.hdf5')\n","# model.load_weights('./SAINT_win0_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hWrldAOqlFEY"},"source":["# Our Code"]},{"cell_type":"markdown","metadata":{"id":"fgZRM7IblIvO"},"source":["## Callbacks"]},{"cell_type":"code","metadata":{"id":"t0OKcoD9qARh","executionInfo":{"status":"ok","timestamp":1637742196522,"user_tz":-360,"elapsed":8,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["def set_es(loss_to_monitor = 'val_truncated_accuracy', patience = 20, verbosity = True):\n","    global es_callback\n","    es_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=loss_to_monitor, patience=patience, verbose=verbosity)\n","\n","set_es()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"h1crcpRrmyw-","executionInfo":{"status":"ok","timestamp":1637742196523,"user_tz":-360,"elapsed":7,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"63fd45bd-5f69-45f8-8cae-898a909a5610"},"source":["# Learning rate\n","import matplotlib.pyplot as plt\n","\n","start_lr = 1e-7\n","min_lr = 1e-5\n","max_lr = 5e-4\n","rampup_epochs = 10\n","sustain_epochs = 0\n","exp_decay = .96\n","d_model = 512\n","\n","current_epoch = 0\n","\n","def lrfn(epoch):\n","    global current_epoch\n","\n","    if current_epoch < rampup_epochs:\n","        lr = (max_lr - start_lr)/rampup_epochs * current_epoch + start_lr\n","        current_epoch += 1\n","        return lr\n","    elif current_epoch < rampup_epochs + sustain_epochs:\n","        current_epoch += 1\n","        return max_lr\n","    else:\n","        lr = (max_lr - min_lr) * exp_decay**(current_epoch-rampup_epochs-sustain_epochs) + min_lr\n","        current_epoch += 1\n","        return lr\n","    # steps_per_epoch = 500\n","    # step = epoch * steps_per_epoch\n","    # warmup_steps = rampup_epochs * steps_per_epoch\n","    # arg1 = step ** -0.5\n","    # arg2 = step * (warmup_steps ** -1.5)\n","\n","    # return (d_model**-0.5) * min(arg1, arg2)\n","\n","lr_callback = tensorflow.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n","\n","rang = np.arange(150)\n","y = [lrfn(x) for x in rang]\n","print(y)\n","plt.plot(rang, y)\n","current_epoch = 0"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1e-07, 5.009000000000001e-05, 0.00010008000000000002, 0.00015007000000000003, 0.00020006000000000003, 0.00025005000000000003, 0.00030004000000000006, 0.00035003000000000004, 0.00040002000000000006, 0.0004500100000000001, 0.0005, 0.00048039999999999997, 0.000461584, 0.0004435206399999999, 0.00042617981439999997, 0.00040953262182399995, 0.0003935513169510399, 0.0003782092642729983, 0.0003634808937020784, 0.00034934165795399524, 0.0003357679916358354, 0.000322737271970402, 0.0003102277810915859, 0.00029821866984792246, 0.00028668992305400554, 0.0002756223261318453, 0.0002649974330865715, 0.0002547975357631086, 0.00024500563433258425, 0.00023560540895928086, 0.00022658119260090962, 0.0002179179448968732, 0.00020960122710099829, 0.00020161717801695834, 0.00019395249089628, 0.0001865943912604288, 0.00017953061561001163, 0.0001727493909856112, 0.00016623941534618672, 0.00015998983873233925, 0.00015399024518304567, 0.00014823063537572386, 0.00014270140996069488, 0.0001373933535622671, 0.0001322976194197764, 0.00012740571464298534, 0.00012270948605726592, 0.00011820110661497527, 0.00011387306235037627, 0.0001097181398563612, 0.00010572941426210676, 0.00010190023769162248, 9.822422818395758e-05, 9.469525905659927e-05, 9.13074486943353e-05, 8.805515074656189e-05, 8.49329447166994e-05, 8.193562692803143e-05, 7.905820185091017e-05, 7.629587377687376e-05, 7.36440388257988e-05, 7.109827727276686e-05, 6.865434618185618e-05, 6.630817233458192e-05, 6.405584544119865e-05, 6.18936116235507e-05, 5.981786715860867e-05, 5.7825152472264316e-05, 5.5912146373373745e-05, 5.407566051843879e-05, 5.231263409770124e-05, 5.0620128733793184e-05, 4.8995323584441456e-05, 4.74355106410638e-05, 4.593809021542125e-05, 4.4500566606804394e-05, 4.312054394253222e-05, 4.179572218483093e-05, 4.0523893297437694e-05, 3.930293756554018e-05, 3.8130820062918575e-05, 3.700558726040183e-05, 3.592536376998576e-05, 3.4888349219186326e-05, 3.389281525041887e-05, 3.293710264040212e-05, 3.201961853478603e-05, 3.113883379339459e-05, 3.0293280441658803e-05, 2.9481549223992448e-05, 2.8702287255032753e-05, 2.795419576483144e-05, 2.7236027934238183e-05, 2.6546586816868657e-05, 2.5884723344193908e-05, 2.524933441042615e-05, 2.4639361034009105e-05, 2.405378659264874e-05, 2.349163512894279e-05, 2.295196972378508e-05, 2.2433890934833674e-05, 2.1936535297440327e-05, 2.1459073885542716e-05, 2.1000710930121005e-05, 2.0560682492916165e-05, 2.0138255193199518e-05, 1.973272498547154e-05, 1.9343415986052673e-05, 1.896967934661057e-05, 1.8610892172746145e-05, 1.8266456485836298e-05, 1.7935798226402847e-05, 1.7618366297346733e-05, 1.7313631645452863e-05, 1.7021086379634746e-05, 1.6740242924449357e-05, 1.6470633207471384e-05, 1.6211807879172527e-05, 1.596333556400563e-05, 1.5724802141445403e-05, 1.5495810055787584e-05, 1.527597765355608e-05, 1.5064938547413839e-05, 1.4862341005517285e-05, 1.4667847365296593e-05, 1.448113347068473e-05, 1.430188813185734e-05, 1.4129812606583046e-05, 1.3964620102319723e-05, 1.3806035298226935e-05, 1.3653793886297858e-05, 1.3507642130845944e-05, 1.3367336445612106e-05, 1.3232642987787622e-05, 1.3103337268276115e-05, 1.297920377754507e-05, 1.2860035626443268e-05, 1.2745634201385537e-05, 1.2635808833330116e-05, 1.253037647999691e-05, 1.2429161420797035e-05, 1.2331994963965153e-05, 1.2238715165406547e-05, 1.2149166558790286e-05, 1.2063199896438674e-05, 1.1980671900581126e-05, 1.1901445024557881e-05, 1.1825387223575566e-05, 1.1752371734632544e-05, 1.1682276865247241e-05]\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhc9X3v8fd3ZrTaWmxt3ndjIxsMtjA7wSbBJgmYFtIYAqHBCc0TyFJoE2huk5Ym9ynl3tCkYQk3JDiUxhBCgpOwJaxhibHAgHcs8G5r8aaRbM9IGv3uH3NkJKFlJI10RtLn9Tx6Zs7203eOLX10zu+c8zPnHCIiIi0CfhcgIiKpRcEgIiJtKBhERKQNBYOIiLShYBARkTZCfheQDIWFhW7KlCl+lyEiMqi8+eabB5xzRe3nD4lgmDJlCuXl5X6XISIyqJjZzo7m61SSiIi0oWAQEZE2FAwiItKGgkFERNpQMIiISBsJBYOZLTWzrWZWYWa3drA8w8we8ZavMbMprZbd5s3famZLumvTzB40s+1m9rb3dVrfPqKIiPREt5ermlkQuBv4BLAHWGtmq51zm1qttgI47JybYWbLgTuAz5pZKbAcmAOMA/5kZid523TV5j865x5LwucTEZEeSuSIYSFQ4Zz7wDnXAKwClrVbZxmw0nv/GHCRmZk3f5VzLuqc2w5UeO0l0uagE22KseqNXTTFmv0uRUSk1xIJhvHA7lbTe7x5Ha7jnGsCaoGCLrbtrs3vm9m7ZnaXmWV0VJSZ3WBm5WZWXlNTk8DH6H8vba3h1sfX86fN1X6XIiLSa6nY+XwbMBs4AxgNfKujlZxz9zvnypxzZUVFH7mj2xeV4QgAr1SkRlCJiPRGIsGwF5jYanqCN6/DdcwsBOQBB7vYttM2nXP7XVwU+Dnx006DQlVLMGw74HMlIiK9l0gwrAVmmtlUM0sn3pm8ut06q4HrvPdXAs+7+Jihq4Hl3lVLU4GZwBtdtWlmY71XAy4HNvTlAw6kytooADsOHmP3oWM+VyMi0jvdBoPXZ3AT8AywGXjUObfRzG43s8u81R4ACsysArgZuNXbdiPwKLAJeBq40TkX66xNr62HzWw9sB4oBL6XnI/a/6rrIuRnpwHwaoWOGkRkcLL4H/aDW1lZmUuFp6t+4gcvMbVwBO/sOULZlNHcffV8v0sSEemUmb3pnCtrPz8VO58HrapwhDF5mZw3o4jXKg7Q3Dz4Q1dEhh8FQ5Icb4gRjjRRkpvJeTMLOHyskY37wn6XJSLSYwqGJGm5IqkkN5NzZxQC8Gddtioig5CCIUkqTwRDBsU5mcwek6MOaBEZlBQMSdJyxDAmNxOA82YUsnbHYSKNMT/LEhHpMQVDkrQEQ3FLMMwspKGpmTe2H/KzLBGRHlMwJElVOEpWWpDczPgDaxdOHU16MMArOp0kIoOMgiFJqsIRSnIziN+wDdnpIeZPzufPejyGiAwyCoYkiQdDZpt5588sYvP+MDV1UZ+qEhHpOQVDklSFox8JhvO8y1b1tFURGUwUDEngnKPSu+u5tVPG51EwIp0XtigYRGTwUDAkQe3xRhqaminOaTumUCBgfGxWES+9V0NMj8cQkUFCwZAELTe3tT9iAFg8u5ja442s23V4oMsSEekVBUMSVIXjncvt+xgg3gEdDBjPb9FwnyIyOCgYkqCq1nscRs5HgyEvK40Fk0cpGERk0FAwJMGHdz1ndLh88exitlTWsb/2+ECWJSLSKwqGJKgMx0duy0wLdrh88exiAF2dJCKDgoIhCarC0RMPz+vIzOKRjM/P0ukkERkUFAxJUF0XOfHwvI6YGYtmF/FqxQE9bVVEUp6CIQkqayOM6aR/ocXi2cUcb4zpaasikvIUDH3UFGvmQP1HH4fR3tnTCskIBXQ6SURSnoKhjw7UN9DsOr6HobWs9CBnTy/gha3VOKe7oEUkdSkY+qj1WM/dWTy7mJ0Hj7H9wNH+LktEpNcUDH1U2W5Iz64smhW/bPW5zTqdJCKpS8HQR9Unjhi67nwGmDg6m5PH5vLMxsr+LktEpNcUDH1UGY4QDBgFI7sPBoAlc0p4c9dhDd4jIilLwdBHVeEoRSMzCAYsofWXzBmDc/DHTVX9XJmISO8oGPqoZaznRM0ek8Ok0dk6nSQiKUvB0EcdjfXcFTNjyZwSXnv/AOFIYz9WJiLSOwqGPuporOfuLJkzhsaY4wXd7CYiKUjB0AeRxhi1xxs7HLmtK/MnjaIoJ4NnN6qfQURST0LBYGZLzWyrmVWY2a0dLM8ws0e85WvMbEqrZbd587ea2ZIetPkjM6vv3ccaGCfGYchJvI8B4mNBf6K0hBe3VuuheiKScroNBjMLAncDlwClwFVmVtputRXAYefcDOAu4A5v21JgOTAHWArcY2bB7to0szJgVB8/W7+rrO18rOfuLJkzhqMNMV6tOJDsskRE+iSRI4aFQIVz7gPnXAOwCljWbp1lwErv/WPARWZm3vxVzrmoc247UOG112mbXmjcCXyzbx+t/1XVdT7Wc3fOnlZATmZIVyeJSMpJJBjGA7tbTe/x5nW4jnOuCagFCrrYtqs2bwJWO+f2d1WUmd1gZuVmVl5T48/IaCfGeu5FMKSHAiyeXcyfNlfTFGtOdmkiIr2WUp3PZjYO+AzwX92t65y73zlX5pwrKyoq6v/iOlAVjpCZFiA3M9Sr7ZfMGcOhow2U7zyc5MpERHovkWDYC0xsNT3Bm9fhOmYWAvKAg11s29n804EZQIWZ7QCyzawiwc8y4CrDEcbkZhI/a9ZzHzupiIxQgCfXd3lwJCIyoBIJhrXATDObambpxDuTV7dbZzVwnff+SuB5Fx90YDWw3LtqaSowE3ijszadc39wzo1xzk1xzk0Bjnkd2impOhztckjP7ozICHHRycU8uX6/TieJSMroNhi8PoObgGeAzcCjzrmNZna7mV3mrfYAUOD9dX8zcKu37UbgUWAT8DRwo3Mu1lmbyf1o/a+qrmd3PXfk0lPHcaC+gTUa8lNEUkRCJ8edc08CT7ab951W7yPE+wY62vb7wPcTabODdUYmUp8fnHNU1ka4uLRn9zC0t2h2MSPSg/zunX2cO6MwSdWJiPReSnU+Dybh401Em5r7fMSQmRbk4jljeGpDJQ1NOp0kIv5TMPRSZQ+G9OzOpfPGUnu8kVcq/LnsVkSkNQVDL/VkrOfunDejiLysNH73jq5OEhH/KRh6qSdjPXcnPRTgkrljeHZjpZ6dJCK+UzD0UstYz8U9GKSnK5fOG8fRhpgexS0ivlMw9FJlOEJ+dhqZacGktHfm1NEUjkznd+/uS0p7IiK9pWDopapwlJKcvp9GahEKBvjkKWN5bnM19dGmpLUrItJTCoZeqgpHKOnF47a7cum8cUSbmvnjJj1xVUT8o2DopapwhJIeDtDTnQWTRjEuL5PfrtPpJBHxj4KhF2LNjpq6no/13J1AwPjr+RP487aaE5fDiogMNAVDLxyoj9LsSPqpJIC/nj+eZge/Wdf+AbYiIgNDwdALJ25uS/KpJIBpRSNZMHkUv35zD/EH1IqIDCwFQy/0ZaznRFwxfwLbqut5d09tv7QvItIVBUMv9GWs50R86tSxpIcC/PqtPf3SvohIVxQMvVBVGyFgUDgy+aeSAPKy0lgyZwxPvL2PaJMekSEiA0vB0AtV4QhFORkEA70b0jMRV8wfT+3xRp7frEdkiMjAUjD0QstYz/3p/JlFlORm6HSSiAw4BUMv9HWs50QEA8blp4/nha011Hh9GiIiA0HB0AsDccQAcOX8CcSaHU+8rXsaRGTgKBh6KNIYo/Z4IyVJetx2V2aW5HDaxHweWbtb9zSIyIBRMPRQdTh+Wqe/TyW1uHrhJLZV17N2x+EB+X4iIgqGHkrmyG2J+PS8seRkhnh4zc4B+X4iIgqGHkrmWM+JyE4PccX8CTy1vpJDRxsG5HuKyPCmYOihqgE+YgC4+sxJNMSaeezN3QP2PUVk+FIw9FBVOEJGKEBuVmjAvudJJTmcMWUUv3xjN83N6oQWkf6lYOihynCUMXmZmPXfXc8dufrMSWw/cJTXPzg4oN9XRIYfBUMPxUduG7jTSC0umTuW/Ow0dUKLSL9TMPRQf4z1nIjMtCBXzp/AsxurqK7T6G4i0n8UDD3gnOuXsZ4TddWZk2hqdvyqXM9PEpH+o2DogfDxJiKNzf02QE93pheN5JzpBTz8l500xZp9qUFEhr6EgsHMlprZVjOrMLNbO1ieYWaPeMvXmNmUVstu8+ZvNbMl3bVpZg+Y2Ttm9q6ZPWZmI/v2EZOnyjuFM1B3PXfkC+dOZV9thKc3VvpWg4gMbd0Gg5kFgbuBS4BS4CozK2232grgsHNuBnAXcIe3bSmwHJgDLAXuMbNgN23+vXNunnPuVGAXcFMfP2PS+HEPQ3sXzS5mckE2P3tlu281iMjQlsgRw0Kgwjn3gXOuAVgFLGu3zjJgpff+MeAii1/PuQxY5ZyLOue2AxVee5226ZwLA3jbZwEpc+F+y1jPA/EAvc4EAsYXzpnCW7uOsG6Xnp8kIsmXSDCMB1rfcrvHm9fhOs65JqAWKOhi2y7bNLOfA5XAbOC/OirKzG4ws3IzK6+pqUngY/RddT+P9ZyoK8smkpMR4mev7vC1DhEZmlKy89k59wVgHLAZ+Gwn69zvnCtzzpUVFRUNSF2VtRHystLITAsOyPfrzMiMEMsXTuTJ9fvZX3vc11pEZOhJJBj2AhNbTU/w5nW4jpmFgDzgYBfbdtumcy5G/BTTFQnUOCCqwhFfTyO19vmzp+Cc4xev64Y3EUmuRIJhLTDTzKaaWTrxzuTV7dZZDVznvb8SeN7FR5ZZDSz3rlqaCswE3uisTYubASf6GC4DtvTtIyZPPBj8PY3UYuLobJbMGcP/rNnFsYYmv8sRkSGk22Dw+gxuAp4hfmrnUefcRjO73cwu81Z7ACgwswrgZuBWb9uNwKPAJuBp4EbnXKyzNgEDVprZemA9MBa4PWmfto+qwtGUCQaAFedNpfZ4I4+/paE/RSR5EnpEqHPuSeDJdvO+0+p9BPhMJ9t+H/h+gm02A+cmUtNAizU7auqjvl6q2t6CyaM4dUIeP3tlO1ctnEQwMLAP9hORoSklO59T0cH6KLFmlzJ9DABmxt9dMJ0PDhzlGd3wJiJJomBIUOUAj9yWqKVzxzCtcAR3v1BBvFtHRKRvFAwJqgqnxj0M7QUDxpcvnM7GfWFeem9g7ucQkaFNwZCgE4/D8OkBel25/LTxjMvL5J4X3ve7FBEZAhQMCaoKRwgYFIxI97uUj0gPBbjhgmm8seMQb2w/5Hc5IjLIKRgSVBWOUDgyg1AwNXfZZ8+YRMGIdO55scLvUkRkkEvN33IpqGWs51SVlR7k+vOm8uLWGjbsrfW7HBEZxBQMCaoORyj2YaznnrjmrMnkZIR01CAifaJgSFBlOMKYvNS5h6EjeVlpfP6cyTy1oZItlWG/yxGRQUrBkIBIY4wjxxopSfEjBoAvnT+NkekhfvDse36XIiKDlIIhAdUt9zCkcB9Di/zsdL50wTSe3VTFu3uO+F2OiAxCCoYEtIz1nGo3t3XmC+dOYVR2Gv9XRw0i0gsKhgS0DOmZSg/Q60pOZhpf/th0XnqvhrU7dF+DiPSMgiEBVWH/x3ruqc+fPYWinAzufGarnqEkIj2iYEhAdV2UjFCAvKw0v0tJWFZ6kJsWzeCN7Yd4teKg3+WIyCCiYEhAZW185Lb4oHKDx/KFExmfn8Wdz+qoQUQSp2BIQFU4Mmj6F1rLCAX5+kUzeWf3Ef6wfr/f5YjIIKFgSEBVOELxIOpfaO2KBROYPSaHf39qC5HGmN/liMggoGDohnMu5cZ67olgwPhfnyplz+HjrHxth9/liMggoGDoRjjSxPHG2KA8ldTivJmFLJ5dzI+fr+BgfdTvckQkxSkYulHtXao6WE8ltfinT87mWGOMHz63ze9SRCTFKRi60TLW82A+YgCYUZzD1Qsn8fCaXVRU1/ldjoikMAVDN1J1rOfe+MbHZ5KdFuR/P7nF71JEJIUpGLrx4V3Pgz8YCkZmcNPiGTy/pZoXtlT7XY6IpCgFQzeqwhFyM0NkpQf9LiUpvnDuVKYXjeC7qzfq8lUR6ZCCoRtV4UhKD+nZU+mhAP+2bC67Dh3jnhff97scEUlBCoZuVA7iexg6c86MQpadNo77Xnyf7QeO+l2OiKQYBUM3qsORIRcMAN/+5MlkhAJ8d/VGPUdJRNpQMHQh1uyorosOqsdtJ6o4N5ObLz6Jl9+r4akNlX6XIyIpRMHQhYNHo8Sa3ZA8YgC49qzJlI7N5fbfbaIu0uh3OSKSIhQMXaiqHTr3MHQkFAzw/b+aS1VdhH9/Svc2iEhcQsFgZkvNbKuZVZjZrR0szzCzR7zla8xsSqtlt3nzt5rZku7aNLOHvfkbzOxnZubb6DhD6R6Gzpw+aRQrzp3Kw2t28VrFAb/LEZEU0G0wmFkQuBu4BCgFrjKz0narrQAOO+dmAHcBd3jblgLLgTnAUuAeMwt20+bDwGzgFCAL+GKfPmEfDJXHYXTnlotnMaUgm289/i5Ho01+lyMiPkvkiGEhUOGc+8A51wCsApa1W2cZsNJ7/xhwkcWHO1sGrHLORZ1z24EKr71O23TOPek8wBvAhL59xN6rDkcIGBSOTPerhAGRlR7kP66cx57Dx7nzma1+lyMiPkskGMYDu1tN7/HmdbiOc64JqAUKuti22za9U0jXAk93VJSZ3WBm5WZWXlNTk8DH6LnKcITCkRmEgkO/K2bh1NFcd/YUHnxtB29sP+R3OSLio1T+jXcP8LJz7s8dLXTO3e+cK3POlRUVFfVLAYN5gJ7e+ObSWUwcncU3H3uH4w16XIbIcJVIMOwFJraanuDN63AdMwsBecDBLrbtsk0z+y5QBNycyIfoL1VD9Oa2zmSnh7jjilPZcfAY3/vDJr/LERGfJBIMa4GZZjbVzNKJdyavbrfOauA67/2VwPNeH8FqYLl31dJUYCbxfoNO2zSzLwJLgKucc819+3h9Ew+GoXdzW1fOmV7IDRdM4+E1u3h2o258ExmOug0Gr8/gJuAZYDPwqHNuo5ndbmaXeas9ABSYWQXxv/Jv9bbdCDwKbCLeV3Cjcy7WWZteW/cBJcDrZva2mX0nSZ+1R6JNMQ4faxzyVyR15B8unsXc8bl869fvnrhkV0SGDxsKz8kpKytz5eXlSW1z96FjnP8fL/AfV5zK35wxsfsNhpj3a+r59I9eYf7kfB66/kwCAfO7JBFJMjN70zlX1n5+Knc+++rEzW1D6JHbPTG9aCTfubSUVysO8tNXPvC7HBEZQAqGTlSeuOt5ePUxtLb8jIksnTOGO5/Zyju7j/hdjogMEAVDJ06M9ZwzPI8YAMyMf7/iFIpzMvnKw29x+GiD3yWJyABQMHSiKhwhPRQgP9u3RzWlhPzsdO753Hxq6qJ8/ZG3iTUP/j4pEemagqETLZeqxp/sMbzNm5jPdy8r5eX3avjRc9v8LkdE+pmCoROVtZFhealqZ65eOIkr5k/gR89v44Wt1X6XIyL9SMHQieq6KMUKhhPMjO9dPpfZY3L5xqq32X3omN8liUg/UTB0wDlHVVhHDO1lpQe575r5OOdYsXKtRn0TGaIUDB2oizZxrCE2rC9V7czkghHce80CPqg5yld/uY6mmK9PLRGRfqBg6ED1MBi5rS/OnVHIv10+lxe31vC9P2z2uxwRSbKQ3wWkosohPtZzMly1cBLvV9fz01e2M71oBNeePcXvkkQkSRQMHagaJkN69tVtnzyZ7QeO8i+/28TE0dlcOKvY75JEJAl0KqkDLY/DKFYfQ5eCAeOHV53OrJIcvvLwW6zbddjvkkQkCRQMHagOR8jJDJGdrgOq7ozMCPHg9WdQODKD6x9cS0V1nd8liUgfKRg6UKlLVXukOCeTh1YsJBgIcO0Db7DvyHG/SxKRPlAwdGC4jfWcDJMLRrDy+jOojzRx7QNr9MA9kUFMwdCB4TbWc7LMGZfH/7uujN2Hj3Ptz9ZQe0w3wIkMRgqGdpqbHdV1Ud3c1ktnTSvgJ9cs4L3Keq55QOEgMhgpGNo5eLSBWLNjzDAduS0ZFs0u5r5r57O1si5+5HBc4SAymCgY2mm5h6F4GA/QkwyLZ5dw7zXz2bw/zOcfUDiIDCYKhnZO3NymI4Y+u+jkEu793AI27Q9zzU/XcLA+6ndJIpIABUM7Gus5uT5eWsJPrl3Ae1V1/M1PXtelrCKDgIKhnapwFDMoGqlgSJbFs0t4aMWZVIejXHnva3xQU+93SSLSBQVDO1W1EQpHZhAKatck08Kpo/nlDWcRbWrmM/e9zoa9tX6XJCKd0G+/dqrqdNdzf5k7Po9ffflsMtOCfPYnr/PCFg0RKpKKFAztVNZG1L/Qj6YVjeTxr5zD1KIRrFi5lpWv7fC7JBFpR8HQjsZ67n8luZk8+ndns3h2Cd9dvZF/Wb2RWLPzuywR8SgYWok2xTh0tEGnkgZAdnqIn1y7gC+eN5UHX9vBF1eu1b0OIilCwdBKdbhl5DadShoIwYDxvz5dyvcun8uftx3gsh+/wpbKsN9liQx7CoZWqus01rMfrjlrMqtuOIvjDTH+6u7XeOLtvX6XJDKsJRQMZrbUzLaaWYWZ3drB8gwze8RbvsbMprRadps3f6uZLemuTTO7yZvnzKywbx+vZzTWs3/Kpozm9189j7njc/n6qrf5199tpKGp2e+yRIalboPBzILA3cAlQClwlZmVtlttBXDYOTcDuAu4w9u2FFgOzAGWAveYWbCbNl8FPg7s7ONn6zGN9eyv4txM/udLZ/G350zh56/u4Ip7X2P7gaN+lyUy7CRyxLAQqHDOfeCcawBWAcvarbMMWOm9fwy4yMzMm7/KORd1zm0HKrz2Om3TObfOObejj5+rV6rCEdJDAfKz0/z49gKkBQP8y2Vz+Mm1C9h9+Bif+tGf+fWbe/wuS2RYSSQYxgO7W03v8eZ1uI5zrgmoBQq62DaRNrtkZjeYWbmZldfU1PRk007FB+jJIJ5p4qclc8bw1NfP55Txedzyq3f4+qp1GttBZIAM2s5n59z9zrky51xZUVFRUtqsDEco0eO2U8bYvCz+50tncfMnTuL37+7n4v98iee3VPldlsiQl0gw7AUmtpqe4M3rcB0zCwF5wMEutk2kzQFXHY5Sosdtp5RgwPjaRTN54sZzGZWdzvUPlvMPv3pH9zyI9KNEgmEtMNPMpppZOvHO5NXt1lkNXOe9vxJ43jnnvPnLvauWpgIzgTcSbHNAOed0xJDC5o7P44mbzuWmRTP4zbq9XHzXSzy9YT/x/2YikkzdBoPXZ3AT8AywGXjUObfRzG43s8u81R4ACsysArgZuNXbdiPwKLAJeBq40TkX66xNADP7mpntIX4U8a6Z/TR5H7dz9dEmjjXEdHNbCssIBfmHJbP4zVfOoWBEBl/+77dYsbKc3YeO+V2ayJBiQ+EvrrKyMldeXt6nNiqq6/j4D17mh8tPY9lpPeoHFx80xZp58LUd/OCP79HsHF9dPJMvnj+VjFDQ79JEBg0ze9M5V9Z+/qDtfE62Ku9xGBrreXAIBQN88fxp/Onmj3HhScXc+cxWPvGDl3l6Q6VOL4n0kYLBo7GeB6dx+Vncd+0CfnH9QjLTAnz5v99k+f1/0UBAIn2gYPBorOfB7YKTinjya+fzvcvnsq26nkt//ArfWLWOnQd157RIT4X8LiBVVIej5GSGyE7XLhmsQsEA15w1mUvnjePeF9/nwde28/t39/M3Z0zka4tn6mhQJEE6YvDER27TL46hIC8rjVsvmc3L/7iIq8+cxK/Kd/OxO1/gu09sYM9hXcEk0h0Fg0djPQ89xbmZ3L5sLs/fciGXzRvHw2t2ceGdL3Lzo29TUV3nd3kiKUvB4KmqjVCs/oUhaeLobO78zDxe/uYirj17Mk+u388n7nqZv3uonHd2H/G7PJGUoxPqQHOzo7ouqiOGIW5cfhbfvXQONy2awYOv7WDlazt4ZmMVC6eM5tqzJ7NkzhjSQ/pbSUTBABw82kBTs1MfwzBRMDKDWy6exQ0XTGPVG7t56C87+eov11GUk8FVCyfxuTMn6f+CDGsKBj68h0G/DIaXnMw0vnTBNFacN5WX3qvhF6/v4L+e38Y9L1SwZM4Yrj5zEmdPKyAQ0GPYZXhRMNA6GNTHMBwFAsai2cUsml3MzoNH+e+/7OTR8j38Yf1+xuVlcvnp47liwQSmF430u1SRAaFg4MPHYeiIQSYXjODbnyrllotn8cdNVfz6rT3c99L73PPi+5w2MZ8r5o/n06eOY9SIdL9LFek3CgbiRwxmUJSjIwaJy0wLcum8cVw6bxzV4Qi/fXsvv35zL//8xEb+9XebOHt6AZ88ZSwXl5ZQMFL/b2RoUTAQD4aCERmkBXVFinxUcW4mN1wwnS+dP42N+8L8Yf1+nly/n9seX8+3f7Oes6YVcMkpY1lSWkKxjjplCFAwEA+GMXn6q0+6ZmbMHZ/H3PF5fHPJLDbvr+OpDfGQ+OffbuCff7uBU8bnsWhWERfOLmbehHyC6riWQUjBAFSGo4zTc3SkB8yM0nG5lI7L5ZaLZ/FeVR1/3FTFi1ur+fELFfzo+QpGZafxsZOKuHBWMedML9DRhAwaCgagOhzh9En5fpchg9hJJTmcVJLDjYtmcORYAy9vO8CLW6p58b0afvv2PgCmF43gnOmFnDO9gLOmFagDW1LWsA+GaFOMg0cbNNazJE1+djqXzRvHZfPGEWt2bNoX5vUPDvDa+wd5/K09PPSXnQDMHpPDgsmjmD9pFPMnj2JKQTZmOvUk/hv2wVBTF79UVX0M0h+CAeOUCXmcMiGPGy6YTmOsmXf31PL6+wdYs/0Qq9/ex8NrdgEwekQ6p0/MZ/7kUZw+KZ95E/IZkTHsf0TFB8P+f13LzW06/ysDIS0YYMHkUSyYPIqbgFizo6K6nrd2HeatnYd5a9dhnttSDUDAYFrRSOaMy6V0bLw/Y864PEbrFJT0MwWDd3ObHqAnfggGjFljcpg1JoerFk4C4MixBtbtPsK6XRiWStoAAAq0SURBVEfYtC/M2u2HeMLrp4D4/9V4SOQysySHGUUjmVY0gsy0oF8fQ4aYYR8MlbV6TpKklvzsdBbNKmbRrOIT8w4fbWDT/jCb9oXZuK+WTfvDvLi1mmYXX24GE0dlM6N4ZJuvaYUjyM/WEYb0zLAPhqq6COnBAKOy0/wuRaRTo0akc+6MQs6dUXhiXqQxxvYDR9lWXU9FdT3ve6+vbDtAQ6z5xHq5mSEmF4xgUkE2k0ZnM3l0NpMKsplcMIKxuZl6SKB8xLAPhupwlOLcDF0NIoNOZlqQk8fmcvLY3Dbzm2LN7D58nIrqenYePMrOg8fYeegYG/fW8syGSppaDjOAUMAoyc1kXH4mY/OyGJufybi8LMbmZTIuP/46ekS6fj6GmWEfDBrrWYaaUDDA1MIRTC0c8ZFlTbFm9tdGvLA4yp7Dx9l/5Dj7aiOs232YpzZEaIy5NttkhAKMycukOCeDwpEZFOVkUOS9npj23mugo6Fh2AdDVV2Ek8fkdr+iyBAQCgaYODqbiaOzOY/CjyxvbnYcOBpl/5EI+2uPs8973V8b4UB9lG3V9bz2/kFqjzd22H5eVhqjstPIz05nVHYao7LTT7zPH9F6Xvx1VHY6mWkBHZGkGAVDbYSPnVTkdxkiKSEQMIpzMinOyWTexM6fBhBtinGgvoEDdVFq6qLU1MdfD9RHOXyskSPHGqipj/JeVT1HjjVwtCHWaVtpQWNkRoiczDTvteWr9XQaIzND5GaGTqw7IiNIVlqQ7PQQWelBstODehBmkgzrYKiPNnG0IaZLVUV6KCMUZHx+FuPzsxJaP9oUo/ZYI4ePNXL4WANHjjVwxJsORxqpjzRRF2mkPtpEONLEviMR6qJ13vymNv0iXQkF7ERIZKUFyUoPke1NZ6YFP/I+IxQkPRQgIxTwXttPx+dldLJORihAaAiG0bAOBl2qKjIwMkJBinODvbqR1DlHtKm5VYA0UR+Nfx1viHGsIcaxhiYijS3vYxxviHHcmz7eGF+3pi764TxveSzBwOlKwOI3LqYFA4SCRigQIC1ohIJGWqD9vAChgJEeir+Ggt58b72W9dOCHy4PBYxgq69Qu/eXzRtPXpKvqhzWwVCtsZ5FUp6ZkZkW/yu/OCe5bTfFmmmINdPQ1Ey0qZloYzMNsRiRxvj86InXGNGmD9draGo73RhrpjHmaGr2XmPNNDU7GmPNNLWa3zJdH22iqWW6Ob5+y/Zt5ztiznUZYOfMKFQwJFOlxnoWGdZCwfipoFS/B9B54dASEk3NjpgXGvlZyb8HK6GTY2a21My2mlmFmd3awfIMM3vEW77GzKa0WnabN3+rmS3prk0zm+q1UeG12W//ZBrrWUQGA7P4aaWMULyzPTczjVEj0ikcmdEvfRzdtmhmQeBu4BKgFLjKzErbrbYCOOycmwHcBdzhbVsKLAfmAEuBe8ws2E2bdwB3eW0d9truF1XhCDkZIT3BUkSklUSiZiFQ4Zz7wDnXAKwClrVbZxmw0nv/GHCRxS9MXgascs5FnXPbgQqvvQ7b9LZZ7LWB1+blvf94XasKRyjRyG0iIm0kEgzjgd2tpvd48zpcxznXBNQCBV1s29n8AuCI10Zn3wsAM7vBzMrNrLympiaBj/FRc8fn8YnSkl5tKyIyVA3acyjOufuB+wHKysp6dc3ZjYtmJLUmEZGhIJEjhr3AxFbTE7x5Ha5jZiEgDzjYxbadzT8I5HttdPa9RESkHyUSDGuBmd7VQunEO5NXt1tnNXCd9/5K4HnnnPPmL/euWpoKzATe6KxNb5sXvDbw2nyi9x9PRER6qttTSc65JjO7CXgGCAI/c85tNLPbgXLn3GrgAeAhM6sADhH/RY+33qPAJqAJuNE5FwPoqE3vW34LWGVm3wPWeW2LiMgAsfgf6YNbWVmZKy8v97sMEZFBxczedM6VtZ8/9J7+JCIifaJgEBGRNhQMIiLShoJBRETaGBKdz2ZWA+zs5eaFwIEkltMfVGNypHqNqV4fqMZkSZUaJzvnPjKE5ZAIhr4ws/KOeuVTiWpMjlSvMdXrA9WYLKleo04liYhIGwoGERFpQ8HgPYgvxanG5Ej1GlO9PlCNyZLSNQ77PgYREWlLRwwiItKGgkFERNoY1sFgZkvNbKuZVZjZrSlQz0Qze8HMNpnZRjP7ujd/tJn90cy2ea+jUqDWoJmtM7Pfe9NTzWyNty8f8R6n7md9+Wb2mJltMbPNZnZ2qu1HM/t77995g5n90swy/d6PZvYzM6s2sw2t5nW43yzuR16t75rZfB9rvNP7t37XzH5jZvmtlt3m1bjVzJb4VWOrZbeYmTOzQm/al/3YlWEbDGYWBO4GLgFKgavMrNTfqmgCbnHOlQJnATd6Nd0KPOecmwk850377evA5lbTdwB3OedmAIeBFb5U9aEfAk8752YD84jXmjL70czGA18Dypxzc4k/fn45/u/HB4Gl7eZ1tt8uIT7GykzgBuBeH2v8IzDXOXcq8B5wG4D387McmONtc4/3s+9HjZjZROBiYFer2X7tx04N22AAFgIVzrkPnHMNwCpgmZ8FOef2O+fe8t7XEf9lNt6ra6W32krgcn8qjDOzCcCngJ960wYsBh7zVvG1RjPLAy7AG8vDOdfgnDtCiu1H4uOhZHkjFmYD+/F5PzrnXiY+pkprne23ZcAvXNxfiI++ONaPGp1zz7YaK/4vxEd/bKlxlXMu6pzbDlQQ/9kf8Bo9dwHfBFpf9ePLfuzKcA6G8cDuVtN7vHkpwcymAKcDa4AS59x+b1ElUOJTWS3+k/h/7mZvugA40uoH0+99ORWoAX7une76qZmNIIX2o3NuL/B/iP/luB+oBd4ktfZji872W6r+DF0PPOW9T5kazWwZsNc59067RSlTY4vhHAwpy8xGAr8GvuGcC7de5g1/6ts1xmb2aaDaOfemXzUkIATMB+51zp0OHKXdaaMU2I+jiP+lOBUYB4ygg1MPqcbv/dYdM/s28VOyD/tdS2tmlg38E/Adv2tJxHAOhr3AxFbTE7x5vjKzNOKh8LBz7nFvdlXLoaX3Wu1XfcC5wGVmtoP46bfFxM/n53unRMD/fbkH2OOcW+NNP0Y8KFJpP34c2O6cq3HONQKPE9+3qbQfW3S231LqZ8jM/hb4NPA59+ENWqlS43TifwS84/3sTADeMrMxpE6NJwznYFgLzPSuAkkn3kG12s+CvHP1DwCbnXM/aLVoNXCd9/464ImBrq2Fc+4259wE59wU4vvseefc54AXgCu91fyusRLYbWazvFkXER93PGX2I/FTSGeZWbb3795SY8rsx1Y622+rgc97V9WcBdS2OuU0oMxsKfHTm5c55461WrQaWG5mGWY2lXgH7xsDXZ9zbr1zrtg5N8X72dkDzPf+r6bMfjzBOTdsv4BPEr+C4X3g2ylQz3nED9PfBd72vj5J/Bz+c8A24E/AaL9r9eq9EPi9934a8R+4CuBXQIbPtZ0GlHv78rfAqFTbj8C/AluADcBDQIbf+xH4JfE+j0biv7xWdLbfACN+Zd/7wHriV1j5VWMF8fP0LT8397Va/9tejVuBS/yqsd3yHUChn/uxqy89EkNERNoYzqeSRESkAwoGERFpQ8EgIiJtKBhERKQNBYOIiLShYBARkTYUDCIi0sb/B0OSMNX4cP2VAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"TqpZcAonlYHD","executionInfo":{"status":"ok","timestamp":1637742196523,"user_tz":-360,"elapsed":5,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["# Checkpoint\n","def set_cp(file = 'best.hdf5', monitor = 'val_loss', verbosity = 1, monitor_mode = 'auto', only_weights = True, best_only = True):\n","    global cp_callback\n","\n","    cp_callback = keras.callbacks.ModelCheckpoint(filepath = save_path + 'best-{val_truncated_accuracy:.6f}.hdf5',\n","                                                monitor = monitor,\n","                                                verbose = verbosity,\n","                                                mode= monitor_mode, \n","                                                save_weights_only = only_weights, \n","                                                save_best_only = best_only)\n","\n","set_cp(monitor = 'val_truncated_accuracy', only_weights=False)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lu-a3iLgFdB0"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"YocbGHqaPUnr","executionInfo":{"status":"ok","timestamp":1637742196523,"user_tz":-360,"elapsed":4,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["model.compile(optimizer=Adam(),\n","              loss='categorical_crossentropy', #https://github.com/LucaAngioloni/ProteinSecondaryStructure-CNN/blob/master/dataset.py\n","              sample_weight_mode='temporal',\n","              metrics=[truncated_accuracy, 'accuracy','mae'])\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltDzI36TVPDU","executionInfo":{"status":"ok","timestamp":1637742199503,"user_tz":-360,"elapsed":2984,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"0fec28ac-d17c-44af-b17f-b4a0d1a4af99"},"source":["trainX = np.load('/content/training-needs/trainX.npy')\n","print(trainX.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["(10029, 700, 57)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SaGWl8wV_8c","executionInfo":{"status":"ok","timestamp":1637742200109,"user_tz":-360,"elapsed":610,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"919f405c-660f-416b-8ad1-f09a78158668"},"source":["trainY = np.load('/content/training-needs/trainY.npy')[:,:,:8]\n","print(trainY.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["(10029, 700, 8)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pXydCrZWiBT","executionInfo":{"status":"ok","timestamp":1637742200110,"user_tz":-360,"elapsed":6,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"31a4ccf8-6276-4488-ba01-27714b685f00"},"source":["trainAttm = np.load('/content/training-needs/trainAttM.npy')\n","print(trainAttm.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["(10029, 700)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__Uy-Sp0W-dC","executionInfo":{"status":"ok","timestamp":1637742200110,"user_tz":-360,"elapsed":4,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"d8f37266-477d-4694-9ca7-6334346cc4db"},"source":["trainPosIds = np.array(range(700))\n","trainPosIds = np.repeat([trainPosIds], int(trainX.shape[0]), axis=0)\n","print(trainPosIds.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["(10029, 700)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"319T5nWxXNwV","executionInfo":{"status":"ok","timestamp":1637742200110,"user_tz":-360,"elapsed":3,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"44ac2460-7bd1-4607-de9e-f2284b42f10b"},"source":["validationX = np.load('/content/training-needs/validationX.npy')\n","print(validationX.shape)\n","validationY = np.load('/content//training-needs/validationY.npy')[:,:,:8]\n","print(validationY.shape)\n","validationAttm = np.load('/content/training-needs/validationAttM.npy')\n","print(validationAttm.shape)\n","validationPosIds = np.array(range(700))\n","validationPosIds = np.repeat([validationPosIds], int(validationX.shape[0]), axis=0)\n","print(validationPosIds.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["(983, 700, 57)\n","(983, 700, 8)\n","(983, 700)\n","(983, 700)\n"]}]},{"cell_type":"code","metadata":{"id":"x820koXBzmE5","executionInfo":{"status":"ok","timestamp":1637742202184,"user_tz":-360,"elapsed":2076,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["trainX = np.float32(trainX)\n","trainAttm = np.float32(trainAttm)\n","trainPosIds = np.float32(trainPosIds)\n","trainY = np.int32(trainY)\n","validationX = np.float32(validationX)\n","validationAttm = np.float32(validationAttm)\n","validationPosIds = np.float32(validationPosIds)\n","validationY = np.int32(validationY)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oP_2gmDk2t51","executionInfo":{"status":"ok","timestamp":1637742202185,"user_tz":-360,"elapsed":6,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}},"outputId":"fbf878ed-99a7-4027-acf0-899593e63fd8"},"source":["print(keras.backend.floatx())"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["float32\n"]}]},{"cell_type":"code","metadata":{"id":"zKsk8EviGUGj","executionInfo":{"status":"ok","timestamp":1637742202185,"user_tz":-360,"elapsed":3,"user":{"displayName":"Aunabil Chakma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-YGWljvpYw8OoVoasAXJdxlmrBOSecFgL4LGd=s64","userId":"09729291263754678162"}}},"source":["total_epochs = 150"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WdtTd6dRQbL","outputId":"f2a53903-fe50-449d-9a51-9e33dbcafe45"},"source":["## run1\n","history = model.fit(\n","    x=[trainX,trainAttm,trainPosIds],\n","    y=trainY,\n","    batch_size=12,\n","    epochs=total_epochs,\n","    verbose=1,\n","    validation_data=([validationX,validationAttm,validationPosIds],validationY),\n","    # validation_batch_size = 8,\n","    shuffle=True,\n","    workers = -1,\n","    callbacks=[lr_callback, es_callback, cp_callback]\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train on 10029 samples, validate on 983 samples\n","Epoch 1/150\n","\n","Epoch 00001: LearningRateScheduler reducing learning rate to 5.009000000000001e-05.\n","10029/10029 [==============================] - 621s 62ms/step - loss: 10.5357 - truncated_accuracy: 0.4068 - accuracy: 0.1811 - mae: 0.1389 - val_loss: 8.8114 - val_truncated_accuracy: 0.5905 - val_accuracy: 0.2085 - val_mae: 0.1240\n","\n","Epoch 00001: val_truncated_accuracy improved from -inf to 0.59053, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.590531.hdf5\n","Epoch 2/150\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.00010008000000000002.\n","10029/10029 [==============================] - 617s 61ms/step - loss: 6.6859 - truncated_accuracy: 0.6176 - accuracy: 0.3025 - mae: 0.1238 - val_loss: 4.9884 - val_truncated_accuracy: 0.6671 - val_accuracy: 0.2272 - val_mae: 0.1196\n","\n","Epoch 00002: val_truncated_accuracy improved from 0.59053 to 0.66706, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.667055.hdf5\n","Epoch 3/150\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.00015007000000000003.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 3.5474 - truncated_accuracy: 0.6716 - accuracy: 0.3542 - mae: 0.1196 - val_loss: 2.4216 - val_truncated_accuracy: 0.6927 - val_accuracy: 0.2878 - val_mae: 0.1187\n","\n","Epoch 00003: val_truncated_accuracy improved from 0.66706 to 0.69274, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.692743.hdf5\n","Epoch 4/150\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.00020006000000000003.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 1.6091 - truncated_accuracy: 0.6942 - accuracy: 0.3415 - mae: 0.1175 - val_loss: 1.0048 - val_truncated_accuracy: 0.7103 - val_accuracy: 0.2767 - val_mae: 0.1166\n","\n","Epoch 00004: val_truncated_accuracy improved from 0.69274 to 0.71030, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.710300.hdf5\n","Epoch 5/150\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.00025005000000000003.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.6856 - truncated_accuracy: 0.7093 - accuracy: 0.3402 - mae: 0.1161 - val_loss: 0.4621 - val_truncated_accuracy: 0.7153 - val_accuracy: 0.2295 - val_mae: 0.1156\n","\n","Epoch 00005: val_truncated_accuracy improved from 0.71030 to 0.71533, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.715326.hdf5\n","Epoch 6/150\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.00030004000000000006.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.3773 - truncated_accuracy: 0.7145 - accuracy: 0.3573 - mae: 0.1157 - val_loss: 0.3181 - val_truncated_accuracy: 0.7147 - val_accuracy: 0.4018 - val_mae: 0.1150\n","\n","Epoch 00006: val_truncated_accuracy did not improve from 0.71533\n","Epoch 7/150\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.00035003000000000004.\n","10029/10029 [==============================] - 614s 61ms/step - loss: 0.3095 - truncated_accuracy: 0.7175 - accuracy: 0.3471 - mae: 0.1154 - val_loss: 0.3123 - val_truncated_accuracy: 0.6963 - val_accuracy: 0.2492 - val_mae: 0.1172\n","\n","Epoch 00007: val_truncated_accuracy did not improve from 0.71533\n","Epoch 8/150\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.00040002000000000006.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.3098 - truncated_accuracy: 0.7170 - accuracy: 0.3742 - mae: 0.1155 - val_loss: 0.3298 - val_truncated_accuracy: 0.6940 - val_accuracy: 0.3226 - val_mae: 0.1190\n","\n","Epoch 00008: val_truncated_accuracy did not improve from 0.71533\n","Epoch 9/150\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.0004500100000000001.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.3161 - truncated_accuracy: 0.7161 - accuracy: 0.3792 - mae: 0.1156 - val_loss: 0.3154 - val_truncated_accuracy: 0.7153 - val_accuracy: 0.3057 - val_mae: 0.1167\n","\n","Epoch 00009: val_truncated_accuracy did not improve from 0.71533\n","Epoch 10/150\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005.\n","10029/10029 [==============================] - 613s 61ms/step - loss: 0.3333 - truncated_accuracy: 0.7128 - accuracy: 0.3851 - mae: 0.1159 - val_loss: 0.3417 - val_truncated_accuracy: 0.6908 - val_accuracy: 0.2890 - val_mae: 0.1188\n","\n","Epoch 00010: val_truncated_accuracy did not improve from 0.71533\n","Epoch 11/150\n","\n","Epoch 00011: LearningRateScheduler reducing learning rate to 0.00048039999999999997.\n","10029/10029 [==============================] - 617s 61ms/step - loss: 0.3417 - truncated_accuracy: 0.7141 - accuracy: 0.4041 - mae: 0.1157 - val_loss: 0.3523 - val_truncated_accuracy: 0.7066 - val_accuracy: 0.3638 - val_mae: 0.1151\n","\n","Epoch 00011: val_truncated_accuracy did not improve from 0.71533\n","Epoch 12/150\n","\n","Epoch 00012: LearningRateScheduler reducing learning rate to 0.000461584.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.3324 - truncated_accuracy: 0.7168 - accuracy: 0.4271 - mae: 0.1155 - val_loss: 0.3146 - val_truncated_accuracy: 0.7161 - val_accuracy: 0.4570 - val_mae: 0.1162\n","\n","Epoch 00012: val_truncated_accuracy improved from 0.71533 to 0.71610, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.716102.hdf5\n","Epoch 13/150\n","\n","Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004435206399999999.\n","10029/10029 [==============================] - 614s 61ms/step - loss: 0.3364 - truncated_accuracy: 0.7170 - accuracy: 0.4342 - mae: 0.1155 - val_loss: 0.3262 - val_truncated_accuracy: 0.7190 - val_accuracy: 0.3787 - val_mae: 0.1162\n","\n","Epoch 00013: val_truncated_accuracy improved from 0.71610 to 0.71904, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.719040.hdf5\n","Epoch 14/150\n","\n","Epoch 00014: LearningRateScheduler reducing learning rate to 0.00042617981439999997.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.3804 - truncated_accuracy: 0.7148 - accuracy: 0.4203 - mae: 0.1157 - val_loss: 0.3536 - val_truncated_accuracy: 0.7114 - val_accuracy: 0.4427 - val_mae: 0.1162\n","\n","Epoch 00014: val_truncated_accuracy did not improve from 0.71904\n","Epoch 15/150\n","\n","Epoch 00015: LearningRateScheduler reducing learning rate to 0.00040953262182399995.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.3319 - truncated_accuracy: 0.7205 - accuracy: 0.4022 - mae: 0.1151 - val_loss: 0.3249 - val_truncated_accuracy: 0.7255 - val_accuracy: 0.3151 - val_mae: 0.1154\n","\n","Epoch 00015: val_truncated_accuracy improved from 0.71904 to 0.72552, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.725517.hdf5\n","Epoch 16/150\n","\n","Epoch 00016: LearningRateScheduler reducing learning rate to 0.0003935513169510399.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.3302 - truncated_accuracy: 0.7217 - accuracy: 0.4262 - mae: 0.1150 - val_loss: 0.3404 - val_truncated_accuracy: 0.7014 - val_accuracy: 0.2657 - val_mae: 0.1158\n","\n","Epoch 00016: val_truncated_accuracy did not improve from 0.72552\n","Epoch 17/150\n","\n","Epoch 00017: LearningRateScheduler reducing learning rate to 0.0003782092642729983.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.3368 - truncated_accuracy: 0.7225 - accuracy: 0.4102 - mae: 0.1150 - val_loss: 0.3752 - val_truncated_accuracy: 0.7080 - val_accuracy: 0.2981 - val_mae: 0.1178\n","\n","Epoch 00017: val_truncated_accuracy did not improve from 0.72552\n","Epoch 18/150\n","\n","Epoch 00018: LearningRateScheduler reducing learning rate to 0.0003634808937020784.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.3532 - truncated_accuracy: 0.7227 - accuracy: 0.4009 - mae: 0.1149 - val_loss: 0.3603 - val_truncated_accuracy: 0.7175 - val_accuracy: 0.4107 - val_mae: 0.1170\n","\n","Epoch 00018: val_truncated_accuracy did not improve from 0.72552\n","Epoch 19/150\n","\n","Epoch 00019: LearningRateScheduler reducing learning rate to 0.00034934165795399524.\n","10029/10029 [==============================] - 617s 61ms/step - loss: 0.3547 - truncated_accuracy: 0.7230 - accuracy: 0.4072 - mae: 0.1149 - val_loss: 0.3195 - val_truncated_accuracy: 0.7265 - val_accuracy: 0.2716 - val_mae: 0.1149\n","\n","Epoch 00019: val_truncated_accuracy improved from 0.72552 to 0.72651, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.726511.hdf5\n","Epoch 20/150\n","\n","Epoch 00020: LearningRateScheduler reducing learning rate to 0.0003357679916358354.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.3379 - truncated_accuracy: 0.7250 - accuracy: 0.4003 - mae: 0.1147 - val_loss: 0.3719 - val_truncated_accuracy: 0.6996 - val_accuracy: 0.3971 - val_mae: 0.1161\n","\n","Epoch 00020: val_truncated_accuracy did not improve from 0.72651\n","Epoch 21/150\n","\n","Epoch 00021: LearningRateScheduler reducing learning rate to 0.000322737271970402.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.3364 - truncated_accuracy: 0.7264 - accuracy: 0.4554 - mae: 0.1146 - val_loss: 0.3188 - val_truncated_accuracy: 0.7249 - val_accuracy: 0.4706 - val_mae: 0.1136\n","\n","Epoch 00021: val_truncated_accuracy did not improve from 0.72651\n","Epoch 22/150\n","\n","Epoch 00022: LearningRateScheduler reducing learning rate to 0.0003102277810915859.\n","10029/10029 [==============================] - 617s 61ms/step - loss: 0.3221 - truncated_accuracy: 0.7284 - accuracy: 0.4522 - mae: 0.1144 - val_loss: 0.2980 - val_truncated_accuracy: 0.7317 - val_accuracy: 0.3263 - val_mae: 0.1150\n","\n","Epoch 00022: val_truncated_accuracy improved from 0.72651 to 0.73169, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.731690.hdf5\n","Epoch 23/150\n","\n","Epoch 00023: LearningRateScheduler reducing learning rate to 0.00029821866984792246.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.3074 - truncated_accuracy: 0.7298 - accuracy: 0.4487 - mae: 0.1142 - val_loss: 0.2911 - val_truncated_accuracy: 0.7337 - val_accuracy: 0.3253 - val_mae: 0.1151\n","\n","Epoch 00023: val_truncated_accuracy improved from 0.73169 to 0.73372, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.733722.hdf5\n","Epoch 24/150\n","\n","Epoch 00024: LearningRateScheduler reducing learning rate to 0.00028668992305400554.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.2979 - truncated_accuracy: 0.7311 - accuracy: 0.4523 - mae: 0.1141 - val_loss: 0.3057 - val_truncated_accuracy: 0.7157 - val_accuracy: 0.5958 - val_mae: 0.1179\n","\n","Epoch 00024: val_truncated_accuracy did not improve from 0.73372\n","Epoch 25/150\n","\n","Epoch 00025: LearningRateScheduler reducing learning rate to 0.0002756223261318453.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2965 - truncated_accuracy: 0.7315 - accuracy: 0.4707 - mae: 0.1141 - val_loss: 0.2911 - val_truncated_accuracy: 0.7317 - val_accuracy: 0.5196 - val_mae: 0.1137\n","\n","Epoch 00025: val_truncated_accuracy did not improve from 0.73372\n","Epoch 26/150\n","\n","Epoch 00026: LearningRateScheduler reducing learning rate to 0.0002649974330865715.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2901 - truncated_accuracy: 0.7325 - accuracy: 0.4603 - mae: 0.1140 - val_loss: 0.2812 - val_truncated_accuracy: 0.7345 - val_accuracy: 0.3193 - val_mae: 0.1138\n","\n","Epoch 00026: val_truncated_accuracy improved from 0.73372 to 0.73452, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.734517.hdf5\n","Epoch 27/150\n","\n","Epoch 00027: LearningRateScheduler reducing learning rate to 0.0002547975357631086.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2900 - truncated_accuracy: 0.7340 - accuracy: 0.4758 - mae: 0.1139 - val_loss: 0.2896 - val_truncated_accuracy: 0.7361 - val_accuracy: 0.4950 - val_mae: 0.1140\n","\n","Epoch 00027: val_truncated_accuracy improved from 0.73452 to 0.73608, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.736075.hdf5\n","Epoch 28/150\n","\n","Epoch 00028: LearningRateScheduler reducing learning rate to 0.00024500563433258425.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2934 - truncated_accuracy: 0.7350 - accuracy: 0.4870 - mae: 0.1137 - val_loss: 0.2782 - val_truncated_accuracy: 0.7369 - val_accuracy: 0.5222 - val_mae: 0.1135\n","\n","Epoch 00028: val_truncated_accuracy improved from 0.73608 to 0.73691, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.736909.hdf5\n","Epoch 29/150\n","\n","Epoch 00029: LearningRateScheduler reducing learning rate to 0.00023560540895928086.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2883 - truncated_accuracy: 0.7366 - accuracy: 0.5036 - mae: 0.1136 - val_loss: 0.2820 - val_truncated_accuracy: 0.7388 - val_accuracy: 0.5223 - val_mae: 0.1132\n","\n","Epoch 00029: val_truncated_accuracy improved from 0.73691 to 0.73882, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.738821.hdf5\n","Epoch 30/150\n","\n","Epoch 00030: LearningRateScheduler reducing learning rate to 0.00022658119260090962.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2812 - truncated_accuracy: 0.7370 - accuracy: 0.5021 - mae: 0.1135 - val_loss: 0.2722 - val_truncated_accuracy: 0.7377 - val_accuracy: 0.5448 - val_mae: 0.1136\n","\n","Epoch 00030: val_truncated_accuracy did not improve from 0.73882\n","Epoch 31/150\n","\n","Epoch 00031: LearningRateScheduler reducing learning rate to 0.0002179179448968732.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2794 - truncated_accuracy: 0.7378 - accuracy: 0.4973 - mae: 0.1135 - val_loss: 0.4135 - val_truncated_accuracy: 0.5863 - val_accuracy: 0.5121 - val_mae: 0.1269\n","\n","Epoch 00031: val_truncated_accuracy did not improve from 0.73882\n","Epoch 32/150\n","\n","Epoch 00032: LearningRateScheduler reducing learning rate to 0.00020960122710099829.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2822 - truncated_accuracy: 0.7382 - accuracy: 0.4946 - mae: 0.1134 - val_loss: 0.2803 - val_truncated_accuracy: 0.7386 - val_accuracy: 0.4710 - val_mae: 0.1139\n","\n","Epoch 00032: val_truncated_accuracy did not improve from 0.73882\n","Epoch 33/150\n","\n","Epoch 00033: LearningRateScheduler reducing learning rate to 0.00020161717801695834.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2825 - truncated_accuracy: 0.7397 - accuracy: 0.4833 - mae: 0.1133 - val_loss: 0.2817 - val_truncated_accuracy: 0.7370 - val_accuracy: 0.3947 - val_mae: 0.1132\n","\n","Epoch 00033: val_truncated_accuracy did not improve from 0.73882\n","Epoch 34/150\n","\n","Epoch 00034: LearningRateScheduler reducing learning rate to 0.00019395249089628.\n","10029/10029 [==============================] - 617s 62ms/step - loss: 0.2794 - truncated_accuracy: 0.7401 - accuracy: 0.4708 - mae: 0.1132 - val_loss: 0.2913 - val_truncated_accuracy: 0.7382 - val_accuracy: 0.4477 - val_mae: 0.1136\n","\n","Epoch 00034: val_truncated_accuracy did not improve from 0.73882\n","Epoch 35/150\n","\n","Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001865943912604288.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2841 - truncated_accuracy: 0.7414 - accuracy: 0.4965 - mae: 0.1131 - val_loss: 0.2763 - val_truncated_accuracy: 0.7420 - val_accuracy: 0.4174 - val_mae: 0.1124\n","\n","Epoch 00035: val_truncated_accuracy improved from 0.73882 to 0.74201, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.742013.hdf5\n","Epoch 36/150\n","\n","Epoch 00036: LearningRateScheduler reducing learning rate to 0.00017953061561001163.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2771 - truncated_accuracy: 0.7422 - accuracy: 0.4962 - mae: 0.1130 - val_loss: 0.2674 - val_truncated_accuracy: 0.7428 - val_accuracy: 0.4525 - val_mae: 0.1136\n","\n","Epoch 00036: val_truncated_accuracy improved from 0.74201 to 0.74280, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.742800.hdf5\n","Epoch 37/150\n","\n","Epoch 00037: LearningRateScheduler reducing learning rate to 0.0001727493909856112.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2731 - truncated_accuracy: 0.7436 - accuracy: 0.5119 - mae: 0.1129 - val_loss: 0.2642 - val_truncated_accuracy: 0.7446 - val_accuracy: 0.4766 - val_mae: 0.1125\n","\n","Epoch 00037: val_truncated_accuracy improved from 0.74280 to 0.74463, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.744632.hdf5\n","Epoch 38/150\n","\n","Epoch 00038: LearningRateScheduler reducing learning rate to 0.00016623941534618672.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2664 - truncated_accuracy: 0.7441 - accuracy: 0.5004 - mae: 0.1128 - val_loss: 0.2565 - val_truncated_accuracy: 0.7427 - val_accuracy: 0.4836 - val_mae: 0.1126\n","\n","Epoch 00038: val_truncated_accuracy did not improve from 0.74463\n","Epoch 39/150\n","\n","Epoch 00039: LearningRateScheduler reducing learning rate to 0.00015998983873233925.\n","10029/10029 [==============================] - 618s 62ms/step - loss: 0.2607 - truncated_accuracy: 0.7452 - accuracy: 0.5084 - mae: 0.1128 - val_loss: 0.2546 - val_truncated_accuracy: 0.7441 - val_accuracy: 0.4111 - val_mae: 0.1124\n","\n","Epoch 00039: val_truncated_accuracy did not improve from 0.74463\n","Epoch 40/150\n","\n","Epoch 00040: LearningRateScheduler reducing learning rate to 0.00015399024518304567.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2607 - truncated_accuracy: 0.7454 - accuracy: 0.4936 - mae: 0.1127 - val_loss: 0.2559 - val_truncated_accuracy: 0.7428 - val_accuracy: 0.5138 - val_mae: 0.1127\n","\n","Epoch 00040: val_truncated_accuracy did not improve from 0.74463\n","Epoch 41/150\n","\n","Epoch 00041: LearningRateScheduler reducing learning rate to 0.00014823063537572386.\n","10029/10029 [==============================] - 616s 61ms/step - loss: 0.2612 - truncated_accuracy: 0.7464 - accuracy: 0.4780 - mae: 0.1126 - val_loss: 0.2517 - val_truncated_accuracy: 0.7465 - val_accuracy: 0.4441 - val_mae: 0.1124\n","\n","Epoch 00041: val_truncated_accuracy improved from 0.74463 to 0.74651, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.746513.hdf5\n","Epoch 42/150\n","\n","Epoch 00042: LearningRateScheduler reducing learning rate to 0.00014270140996069488.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.2577 - truncated_accuracy: 0.7474 - accuracy: 0.4967 - mae: 0.1125 - val_loss: 0.2538 - val_truncated_accuracy: 0.7468 - val_accuracy: 0.4364 - val_mae: 0.1127\n","\n","Epoch 00042: val_truncated_accuracy improved from 0.74651 to 0.74681, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.746812.hdf5\n","Epoch 43/150\n","\n","Epoch 00043: LearningRateScheduler reducing learning rate to 0.0001373933535622671.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.2602 - truncated_accuracy: 0.7476 - accuracy: 0.4790 - mae: 0.1125 - val_loss: 0.2507 - val_truncated_accuracy: 0.7480 - val_accuracy: 0.5403 - val_mae: 0.1121\n","\n","Epoch 00043: val_truncated_accuracy improved from 0.74681 to 0.74800, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.747995.hdf5\n","Epoch 44/150\n","\n","Epoch 00044: LearningRateScheduler reducing learning rate to 0.0001322976194197764.\n","10029/10029 [==============================] - 615s 61ms/step - loss: 0.2540 - truncated_accuracy: 0.7491 - accuracy: 0.5073 - mae: 0.1124 - val_loss: 0.2457 - val_truncated_accuracy: 0.7499 - val_accuracy: 0.5534 - val_mae: 0.1121\n","\n","Epoch 00044: val_truncated_accuracy improved from 0.74800 to 0.74985, saving model to /content/drive/MyDrive/bio-data/AN1-saint-base-build-MHA-8-CLB-FinalRun-1/best-0.749854.hdf5\n","Epoch 45/150\n","\n","Epoch 00045: LearningRateScheduler reducing learning rate to 0.00012740571464298534.\n","10029/10029 [==============================] - 614s 61ms/step - loss: 0.2524 - truncated_accuracy: 0.7498 - accuracy: 0.5107 - mae: 0.1123 - val_loss: 0.2477 - val_truncated_accuracy: 0.7481 - val_accuracy: 0.4968 - val_mae: 0.1119\n","\n","Epoch 00045: val_truncated_accuracy did not improve from 0.74985\n","Epoch 46/150\n","\n","Epoch 00046: LearningRateScheduler reducing learning rate to 0.00012270948605726592.\n","10029/10029 [==============================] - 614s 61ms/step - loss: 0.2510 - truncated_accuracy: 0.7511 - accuracy: 0.5021 - mae: 0.1122 - val_loss: 0.2488 - val_truncated_accuracy: 0.7497 - val_accuracy: 0.3998 - val_mae: 0.1120\n","\n","Epoch 00046: val_truncated_accuracy did not improve from 0.74985\n","Epoch 47/150\n","\n","Epoch 00047: LearningRateScheduler reducing learning rate to 0.00011820110661497527.\n"," 9588/10029 [===========================>..] - ETA: 26s - loss: 0.2543 - truncated_accuracy: 0.7511 - accuracy: 0.4885 - mae: 0.1122"]}]},{"cell_type":"markdown","metadata":{"id":"nepLM93MSzdy"},"source":["**Inference**"]},{"cell_type":"code","metadata":{"id":"bLPhlLP5UUk3"},"source":["############## change validation set to other set for validations\n","############## \n","\n","predX = np.load('/content/training-needs/validationX.npy')\n","print(predX.shape)\n","predY = np.load('/content/training-needs/validationY.npy')[:,:,:8]\n","print(predY.shape)\n","predAttm = np.load('/content/training-needs/validationAttM.npy')\n","print(predAttm.shape)\n","predPosIds = np.array(range(700))\n","predPosIds = np.repeat([predPosIds], int(predX.shape[0]), axis=0)\n","print(predPosIds.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P-3zdLnpYUYi"},"source":["# predictions = model.predict([validationX,validationAttm,validationPosIds],verbose = 1)\n","model.evaluate([predX,predAttm,predPosIds], predY,verbose = 1)"],"execution_count":null,"outputs":[]}]}